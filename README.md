
# RL_mimic_CDE_stable

This repository provides the implementation for my paper **"Stable Representation Learning for Offline RL with Controlled Differential Equations in ICU Time Series"**, accepted to **IJCAI 2025 (AI4TS Workshop)**.

> üìÑ [Paper link ‚Äì coming soon]

A stable training framework was proposed for learning clinically meaningful state representations from irregular ICU time series using Neural Controlled Differential Equations (CDEs), and demonstrate improved offline reinforcement learning performance on the MIMIC-III sepsis treatment cohort.

---
## üìñ Citation & Acknowledgments

This repository builds upon prior work from:

```bibtex
@inproceedings{killian2020empirical,
  title={An Empirical Study of Representation Learning for Reinforcement Learning in Healthcare},
  author={Killian, Taylor W and Zhang, Haoran and Subramanian, Jayakumar and Fatemi, Mehdi and Ghassemi, Marzyeh},
  booktitle={Machine Learning for Health},
  pages={139--160},
  year={2020},
  organization={PMLR}
}
```
with [repo](https://github.com/MLforHealth/rl_representations/tree/main).

## üîß Project Structure

RL_mimic_CDE_stable/

|‚îÄ‚îÄ configs/ # Experiment and model configuration files

|‚îÄ‚îÄ scripts/ # Training and evaluation scripts

|‚îÄ‚îÄ slurm_scripts/ # SLURM job submission templates (for HPC)

|‚îÄ‚îÄ environment.yml # Conda environment file

|‚îÄ‚îÄ requirements.txt # Python dependencies

|‚îÄ‚îÄ README.md

|‚îÄ‚îÄ process_mimic_data # preprocess MIMIC-III data

---

## üß™ Replicating the Experiments

Below is a step by step manual to re-implement the experiment pipeline.

### 0Ô∏è‚É£ Environment Setup

Recommend using Python 3.8+ in a virtual environment. To install dependencies:

```bash
# Create and activate virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt
```

---

### 1Ô∏è‚É£ Data Processing

The [**MIMIC-III v1.4** ICU dataset](https://physionet.org/content/mimiciii/1.4/) (Johnson, A., Pollard, T., & Mark, R. (2016). MIMIC-III Clinical Database (version 1.4). PhysioNet. RRID:SCR_007345. https://doi.org/10.13026/C2XW26) was used to extract a cohort of septic patients based on Sepsis-3 criteria. The data extraction and cohort definition process builds on the open-source pipeline from [microsoft/mimic_sepsis](https://github.com/microsoft/mimic_sepsis/tree/main), with the following key modifications:

- ‚úÖ **Google BigQuery integration**: We replaced the original Postgres-based data access with queries using **Google Cloud BigQuery**, making it easier to use cloud-hosted MIMIC datasets.
- ‚úÖ **Bug fixes and minor revisions**: Several small issues in the original scripts were fixed to ensure compatibility and clean execution across environments.

The adapted code is located in the `process_mimic_data/` folder. 
```bash
cd process_mimic_data
```

It follows a two-step process:

1. Extracts relevant bigquery tables from MIMIC-III into intermediate CSVs.
    ```bash
    python preprocess.py
    ```
    Running will be taking > 1 hour
2. Constructs the final sepsis cohort using Sepsis-3 rules and saves the patient trajectories.
    ```bash
    python sepsis_cohort.py
    ```
    Running will be taking ~2 hour

    > ‚ö†Ô∏è **Note**: To run these scripts, you must have access to MIMIC-III and ensure the required reference files are available (`Reflabs.tsv`, `Refvitals.tsv`, `sample_and_hold.csv` under `process_mimic_data/ReferenceFiles`).

    The 3 resulting csv files will be saved in `data/sepsis_mimiciii/` 
3. Go back to root dir
    ```bash
    cd ..
    ```
    - Then Run ```scripts/compute_acuity_scores.py``` to compute additional acuity scores with the raw features.
    - Run ```scripts/split_sepsis_cohort.py``` to compose a training, and validation split on cohorts, and organize the patient data into trajectory formats.

---

### 2Ô∏è‚É£ Behaviour Cloning 
A behaviour policy will be used in both dBCQ training (as an action elimination baseline) and WIS evaluation. Behaviour cloning is trained using a 2 layer fully connected neural network.

1. Run ```scripts/create_buffers.py``` to create replay buffers. The saving location is setup in ```configs/config_behavCloning.yaml``` in train_buffer and val_buffer fields.

2. Run ```python slurm_scripts/slurm_build_BC.py``` to create args lines into ```slurm_scripts/slurm_bc_exp```.
3. Run ```bash slurm_scripts/slurm_bc_exp ```, this will automatically run ```scripts/train_behavCloning_with_command_line_args.py``` with each args line inside ```slurm_scripts/slurm_bc_exp```. As a result, behaviour cloning policy folders will be saved under ```data/behaviour_clone/```.
4. After getting all behaviour cloning policies under ```data/behaviour_clone/```, you can use a notebook to compare their validation losses to pick the best performing behaviour cloning policy, and save its path into ```configs/common.yaml``` inside behav_policy_file field to be used in later dBCQ training.

Note that as an alternative, you can also run ```scripts/train_behavCloning_with_config_file.py``` to train behaviour cloning policies, however, that will need you to update ```configs/config_behavCloning.yaml``` to include the parameters each time you run.

---

### 3Ô∏è‚É£ Learning CDE State Representations
1. Run ```python slurm_scripts/slurm_build_cde.py```, this will generate ```slurm_scripts/cde_exp_params.txt``` including arguments used to train cde autoencoders.
2. To train state representations, you can run ```bash slurm_scripts/slurm_cde_exp```, this will directly run the batch of experiments with different args. However, note that for different train-test split, different behaviour cloning policy, or different hidden size, the most proper early stopping criteria thresholds might be different. They can be adjusted in ```scripts/experiment.py```:
    ```bash
    self.epsilon_1 = 0.05  # near-optimal val loss tolerance for early stopping criteria
    self.epsilon_2 = 0.08  # plateau variation tolerance for early stopping criteria
    self.plateau_epochs = 20
    self.check_early_stop_epochs = 60 # Number of epochs to check for early stopping criteria
    self.corr_threshold = 0.6  # threshold for correlation loss to be considered significant    
    ```
3. The state representation files including evaluation results and losses will be saved under ```test/``` folder.
---
### 4Ô∏è‚É£ Offline RL Policy Learning and Evaluation
RL policies are learned using the discretized form of [Batch Constrained Q-learning](https://github.com/sfujim/BCQ) from [Fujimoto, et al (2019)](https://arxiv.org/abs/1910.01708). These policies are learned as the final part of the previous step. 
If you want to train autoencoders and RL policies separately, you can go to `scripts/train_model` and comment out `experiment.train_dBCQ_policy(params['pol_learning_rate'])` and add it back later.
The policies are evaluated intermittently using a form of Weighted Importance Sampling, which is a commonly used evaluation in offline RL. 

---
## üë§ Authors

**Yue Gao**  
[GitHub Profile](https://github.com/GAOYUEtianc)  
If you have any questions, please don't hesitate to reach out via [email](mailto:gao12@ualberta.ca)

---

## üìÑ License

This project is licensed under the [MIT License]((https://opensource.org/licenses/MIT)).
